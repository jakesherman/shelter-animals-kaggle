{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the engineered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_pickle(file_name):\n",
    "    f = open(file_name, 'rb')\n",
    "    p = cPickle.load(f)\n",
    "    f.close()\n",
    "    return p\n",
    "\n",
    "\n",
    "train = np.array(read_pickle('data/train.engineered'))\n",
    "test = np.array(read_pickle('data/test.engineered'))\n",
    "outcomes = read_pickle('data/outcomes.engineered')\n",
    "outcomes_le = read_pickle('data/outcomes_le.engineered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for nested cross-validation, model selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nested_cv(X, y, estimator, params, scoring = 'log_loss', cv = 5, \n",
    "              n_jobs = -1, verbose = True):\n",
    "    \"\"\"Performs 5-fold nested cross-validation ([cv] folds in each loop) on an \n",
    "    estimator given a parameter grid of hyperparamaters to optimize over using \n",
    "    grid search.\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "    inner_loop = GridSearchCV(estimator, params, cv = cv, n_jobs = n_jobs, \n",
    "    scoring = scoring)\n",
    "    score = np.absolute(np.mean(cross_val_score(inner_loop, X, y, cv = cv, \n",
    "        n_jobs = 1)))\n",
    "    if verbose:\n",
    "        time_elapsed = time() - start_time\n",
    "        print 'Model:', estimator\n",
    "        print 'Score:', score\n",
    "        print 'Time elapsed:', round(time_elapsed / 60, 1), '\\n'\n",
    "    return score\n",
    "\n",
    "\n",
    "def model_selection(X, y, estimators_params, scoring = 'log_loss', cv = 5, \n",
    "                    n_jobs = -1, refit = True, higher_is_better = False, \n",
    "                    verbose = True):\n",
    "    \"\"\"Evalute multiple estimators using nested cross-validation. If refit is \n",
    "    True, the best scoring estimator is returned as part of a [cv]-fold \n",
    "    GridSearchCV estimator,such that fitting that model with X, y will find the \n",
    "    optimal hyperparameters and return a final model that can be used to make \n",
    "    predictions. [estimators_parms] is a dictionary where the key is the \n",
    "    estimator and the value is the hyperparameter grid for that estimator.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for estimator, params in estimators_params.items():\n",
    "        try:\n",
    "            score = nested_cv(X, y, estimator, params, scoring = scoring, \n",
    "                              cv = cv, n_jobs = n_jobs, verbose = verbose)\n",
    "            scores.append([score, estimator])\n",
    "        except:\n",
    "            if verbose:\n",
    "                print 'The following model failed to produced a nested-cv result:'\n",
    "                print estimator, '\\n'\n",
    "    scores = sorted(scores, reverse = higher_is_better)\n",
    "    best_model = scores[0][1]\n",
    "    if refit:\n",
    "        return GridSearchCV(best_model, estimators_params[best_model], \n",
    "                            cv = cv, n_jobs = n_jobs, scoring = scoring)\n",
    "    else:\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a bunch of different classifiers, from a baseline model like logistic regression all the way up to a model we expect to perform well like random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators_params = {\n",
    "    \n",
    "    # Logistic regression, good baseline model\n",
    "    LogisticRegression(random_state = 50) :  {\n",
    "            'penalty': ['l1', 'l2'], \n",
    "            'class_weight': [None, 'balanced'],\n",
    "            'C': np.logspace(-3, 3, 7)\n",
    "    },\n",
    "    \n",
    "    # SGD logsitic regression w/ elastic net regularization - likely won't perform\n",
    "    # as well as LR b/c it's using SGD, but I want to see if elasticnet helps\n",
    "    SGDClassifier(random_state = 50, loss = 'log', penalty = 'elasticnet') : {\n",
    "            'alpha': np.logspace(-3, 3, 7)\n",
    "    },\n",
    "    \n",
    "    # SVM w/ a linear kernel - LinearSVC is faster than SVC for this kernel\n",
    "    LinearSVC(random_state = 50) : {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'C': np.logspace(-3, 3, 7)\n",
    "    },\n",
    "    \n",
    "    # SVM w/ RBF kernel\n",
    "    SVC(random_state = 25, probability = True, kernel = 'rbf') : {\n",
    "        'C': np.logspace(-3, 3, 7), \n",
    "        'gamma': np.logspace(-6, 3, 10), \n",
    "        'class_weight' : [None, 'balanced']\n",
    "    },\n",
    "    \n",
    "    # SVM w/ polynomial kernel\n",
    "    SVC(random_state = 25, probability = True, kernel = 'poly') : {\n",
    "        'C': np.logspace(-3, 3, 7), \n",
    "        'degree': [2, 3, 4, 5], \n",
    "        'coef0': [0, 1],\n",
    "        'class_weight' : [None, 'balanced']\n",
    "    },\n",
    "    \n",
    "    # Random forest, others in this challenge have had success w/ this algorithm\n",
    "    RandomForestClassifier(random_state = 25, n_estimators = 1000) : {\n",
    "        'max_features' : ['sqrt', 'log2'],\n",
    "        'max_depth' : [3, None],\n",
    "        'min_samples_split': [1, 2, 3, 7],\n",
    "        'min_samples_leaf': [1, 3, 7],\n",
    "        'class_weight' : [None, 'balanced']\n",
    "    },\n",
    "    \n",
    "    # ExtraTreesClassifier - using same params as the random forest\n",
    "    ExtraTreesClassifier(random_state = 25, n_estimators = 1000) : {\n",
    "        'max_features' : ['sqrt', 'log2'],\n",
    "        'max_depth' : [3, None],\n",
    "        'min_samples_split': [1, 2, 3, 7],\n",
    "        'min_samples_leaf': [1, 3, 7],\n",
    "        'class_weight' : [None, 'balanced']\n",
    "    }\n",
    "}\n",
    "\n",
    "best_performing_model = model_selection(train, outcomes, estimators_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
