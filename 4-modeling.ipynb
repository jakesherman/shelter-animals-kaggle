{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake/miniconda2/envs/shelter-animals/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the engineered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_pickle(file_name):\n",
    "    f = open(file_name, 'rb')\n",
    "    p = cPickle.load(f)\n",
    "    f.close()\n",
    "    return p\n",
    "\n",
    "\n",
    "train = read_pickle('data/train.engineered')\n",
    "test = read_pickle('data/test.engineered')\n",
    "outcomes = read_pickle('data/outcomes.engineered')\n",
    "outcomes_le = read_pickle('data/outcomes_le.engineered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the `train` data into training/test sets using the hold-out method. Though there is a DataFrame labeled `test`, this is really the set that we want to make predictions against (and, we don't have labeled examples for this set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    np.array(train), outcomes, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train) == list(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "Though I suspect other models will make more accurate predictions, let me quickly try out a logistic regression model w/ different regularization hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_model(model, hyperparameters, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Given a [model] and a set of possible [hyperparameters], along with \n",
    "    matricies corresponding to hold-out cross-validation, returns a model w/ \n",
    "    optimized hyperparameters using log-loss scoring and 5-fold cross-validation.\n",
    "    \"\"\"\n",
    "    optimized_model = GridSearchCV(\n",
    "        model, hyperparameters, cv = 5, n_jobs = -1, scoring = 'log_loss')\n",
    "    optimized_model.fit(X_train, y_train)\n",
    "    print 'Optimized parameters:', optimized_model.best_params_\n",
    "    print 'Log loss:', np.absolute(optimized_model.score(X_test, y_test))\n",
    "    return optimized_model\n",
    "\n",
    "\n",
    "def create_submission(name, model, train, outcomes, outcomes_le, test):\n",
    "    \"\"\"\n",
    "    Train [model] on [train] and predict the probabilties on [test], and\n",
    "    format the submission according to Kaggle.\n",
    "    \"\"\"\n",
    "    clf = model.best_estimator_\n",
    "    clf.fit(np.array(train), outcomes)\n",
    "    probs = clf.predict_proba(np.array(test))\n",
    "    results = pd.DataFrame(probs)\n",
    "    results.columns = list(outcomes_le.inverse_transform(list(results)))\n",
    "    results['ID'] = pd.read_csv('data/test.csv')[['ID']].astype(int)\n",
    "    results = results[['ID', 'Adoption', 'Died', 'Euthanasia', \n",
    "                       'Return_to_owner', 'Transfer']]\n",
    "    results.to_csv('submissions/' + name, index = False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: {'penalty': 'l1', 'C': 1}\n",
      "Log loss: 0.887987793278\n",
      "CPU times: user 11.1 s, sys: 72 ms, total: 11.2 s\n",
      "Wall time: 1min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake/miniconda2/envs/shelter-animals/lib/python2.7/site-packages/sklearn/grid_search.py:418: ChangedBehaviorWarning: The long-standing behavior to use the estimator's score function in GridSearchCV.score has changed. The scoring parameter is now used.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit_model = train_test_model(\n",
    "    LogisticRegression(), \n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}, \n",
    "    X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission('first_submission.csv', logit_model, \n",
    "                  train, outcomes, outcomes_le, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My estimate of the test error was much lower than the actual error - the log loss on the public leaderboard for this model is 1.83, compared to 0.89 here. Let's also try a logistic regression using an elastic net penalty instead of an L1 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: {'alpha': 0.001}\n",
      "Log loss: 0.898063749436\n",
      "CPU times: user 2.16 s, sys: 44 ms, total: 2.21 s\n",
      "Wall time: 9.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit_en_model = train_test_model(\n",
    "    SGDClassifier(penalty = 'elasticnet', loss = 'log'), \n",
    "    {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}, \n",
    "    X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result was a higher error, time to try a more advanced model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters: {'n_estimators': 2000}\n",
      "Log loss: 0.82965091448\n",
      "CPU times: user 1min 9s, sys: 928 ms, total: 1min 9s\n",
      "Wall time: 7min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_model = train_test_model(\n",
    "    RandomForestClassifier(random_state = 1),\n",
    "    {'n_estimators': [100, 500, 800, 1000, 1500, 2000]},\n",
    "    X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_submission('second_submission.csv', rf_model, \n",
    "                  train, outcomes, outcomes_le, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
